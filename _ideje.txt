Ta dokument vsebuje možne ideje in nadaljnje delo, èe nam ostane še kaj èasa
----------------------------------------------------------------------------

- exploration parameter (C, epsilon, ...) linearna funkcija naslednjih spremenljivk:
	*število avtomatov/akcij/banditov
	*število vseh potegov (max_pulls), se spremeni z resetom
	*trenutno število potegov (tako lahko funkcija se linearno zmanjšuje od zaèetka do konca, na primer)
	*število potegov od zadnjega reseta zaradi changePointDetection ... ta je lahko posebej ali tudi enaka zgornji, da se izognemo še enemu parametru
		- ???: križni testi, da ni overfittanja!
			* vzemi npr. pol primerov (pazi, da je enak delež tistih s fiksnimi avtomati in s spremenljivimi) in tam optimiziraj, na ostalih 5 pa evalviraj (mogoèe dodaj še 2 primera, da boš imel parno število obeh tipov)
			* ponovi npr. najboljših 5 rešitev iz prejšnje toèke, in naredi npr. 10 križnih testov ~ 50 optimizacij in meritev

- hevristike, ki veljajo v splošnem (razmisli kako bi jih kvantificiral/predstavil)
	- ob spremembi okolja obièajno postane najboljši en drugi bandit (trenutno ni situacije, ko bi najboljši postal še boljši) ... to je zelo splošna heuristika, ki je pogosto najbolj verjetna, saj se je avtomat naèeloma nauèil izbirat najboljšega v preteklem intervalu
 
- hevristike, ki pridejo prav za tekmovanje (razmisli kako bi jih kvantificiral/predstavil)
	- ob spremembi okolja se obièajno spremeni veè kot en bandit (lahko izmeriš iz primerov povpreèno število banditov, ki se spremeni na enkrat, izraziš kot verjetnost, da se je spremenil veè kot eden - normaliziraš med veè kot eden in vsi)
	- ta je taka "bosanska": sprememba okolja se pogosteje zgodi ob okroglih/celih številih (zaradi roènega nastavljanja intervalov v testnih primerih)
	- število sprememb je relativno majhno glede na skupno število potegov
		* TOM: dodaj kot feature posameznega testCase-a (in mogoèe uporabi kot vhodno spremenljivko za parametre uèenja) neko mero za "stacionarnost" primera (poglej èlanke), sicer pa sem si zamislu lastno, da gledaš koliko izgubiš v povpreèju na potezo, èe po vsaki spremembi avtomatov slediš prejšnjemu najboljšemu vse do konca, al nekaj podobnega
	- verjetnosti za nagrade so zelo nizke, blizu 0, kar pomeni, da imamo sparse vektorje vzorcev (zelo malo informacije)
		
- poskusi ensamble of algorithms
	*glede na izmerjene rezulate po posameznih primerih ugotovi ali se splaèa sploh uporabit kaj drugega kot UCB1tuned
	*èe ja, potem naredi nek "voting system" na primer treh algoritmov, kjer se izbere tisti machine/akcija, ki ima najveè glasov, sicer pa èe imajo vsi enako naj prevlada npr. UCB1tuned
	
(vprašljivo)
- kaj pa èe opravimo zaporedno K potegov (npr. 5) od vsakega bandita, ki ima manj kot K potegov v zandjih N (npr. 1000) potegih (mehki reset)
	*ali pa še bolj enostavno: vsakiè ko izberemo bandita, ga potegnemo veèkrat zaporedoma:
		število zaporednih potegov pa je doloèeno (1/P + B) * P, pri èemer je P maximalno število potegov, B pa nastavljiv parameter (kolikšen delež VEÈ zaporednih potegov, glede na število vseh potegov), ker je številka zelo majhna (ranga 0.00003 - 0.002) lahko preuredim enaèbo v (1/P + B/1000)*P (tako je v rangu 0.03 - 2.00)

(vprašljivo)		
- v primeru, ko imaš ogromno nièel in le nekaj enic, smatraš, da ti stroj vraèa povpreèno vrednost vseh preteklih rewardov (delaš dejansko prediction), ali je to smiselno ?? kaj bi se zgodilo?


